{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation(x, deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"MIMOCompiled.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>h1real</th>\n",
       "      <th>h1imag</th>\n",
       "      <th>h2real</th>\n",
       "      <th>h2imag</th>\n",
       "      <th>y1real</th>\n",
       "      <th>y1imag</th>\n",
       "      <th>y2real</th>\n",
       "      <th>y2imag</th>\n",
       "      <th>n1real</th>\n",
       "      <th>...</th>\n",
       "      <th>ymod2real</th>\n",
       "      <th>ymod2img</th>\n",
       "      <th>ymod3real</th>\n",
       "      <th>ymod3img</th>\n",
       "      <th>ymod4real</th>\n",
       "      <th>ymod4img</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.380188</td>\n",
       "      <td>0.209425</td>\n",
       "      <td>1.296753</td>\n",
       "      <td>0.849082</td>\n",
       "      <td>1.683406</td>\n",
       "      <td>-1.002812</td>\n",
       "      <td>-0.727922</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>0.923215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>1.302034</td>\n",
       "      <td>-0.727922</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>0.372573</td>\n",
       "      <td>-1.747136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1.597246</td>\n",
       "      <td>0.770869</td>\n",
       "      <td>0.609649</td>\n",
       "      <td>-0.253641</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>1.302034</td>\n",
       "      <td>0.372573</td>\n",
       "      <td>-1.747136</td>\n",
       "      <td>-0.884864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609621</td>\n",
       "      <td>1.302034</td>\n",
       "      <td>-0.727922</td>\n",
       "      <td>-0.006522</td>\n",
       "      <td>0.372573</td>\n",
       "      <td>-1.747136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.225401</td>\n",
       "      <td>-0.091873</td>\n",
       "      <td>-0.924675</td>\n",
       "      <td>0.518831</td>\n",
       "      <td>-0.092323</td>\n",
       "      <td>0.979530</td>\n",
       "      <td>-0.678165</td>\n",
       "      <td>0.607294</td>\n",
       "      <td>-0.525668</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266874</td>\n",
       "      <td>0.063812</td>\n",
       "      <td>-0.678165</td>\n",
       "      <td>0.607294</td>\n",
       "      <td>-1.035015</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.306596</td>\n",
       "      <td>0.085087</td>\n",
       "      <td>0.242272</td>\n",
       "      <td>0.803508</td>\n",
       "      <td>1.266874</td>\n",
       "      <td>0.063812</td>\n",
       "      <td>-1.035015</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>1.485875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.266874</td>\n",
       "      <td>0.063812</td>\n",
       "      <td>-0.678165</td>\n",
       "      <td>0.607294</td>\n",
       "      <td>-1.035015</td>\n",
       "      <td>0.060759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2.530309</td>\n",
       "      <td>-0.485622</td>\n",
       "      <td>1.958288</td>\n",
       "      <td>0.333530</td>\n",
       "      <td>3.692287</td>\n",
       "      <td>-0.333293</td>\n",
       "      <td>-0.425962</td>\n",
       "      <td>-1.103060</td>\n",
       "      <td>1.378002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703411</td>\n",
       "      <td>0.909991</td>\n",
       "      <td>-0.425962</td>\n",
       "      <td>-1.103060</td>\n",
       "      <td>3.286375</td>\n",
       "      <td>0.475154</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target    h1real    h1imag    h2real    h2imag    y1real    y1imag  \\\n",
       "0       0  0.380188  0.209425  1.296753  0.849082  1.683406 -1.002812   \n",
       "1       0 -1.597246  0.770869  0.609649 -0.253641  0.609621  1.302034   \n",
       "2       1  0.225401 -0.091873 -0.924675  0.518831 -0.092323  0.979530   \n",
       "3       0 -0.306596  0.085087  0.242272  0.803508  1.266874  0.063812   \n",
       "4       1  2.530309 -0.485622  1.958288  0.333530  3.692287 -0.333293   \n",
       "\n",
       "     y2real    y2imag    n1real     ...       ymod2real  ymod2img  ymod3real  \\\n",
       "0 -0.727922 -0.006522  0.923215     ...        0.609621  1.302034  -0.727922   \n",
       "1  0.372573 -1.747136 -0.884864     ...        0.609621  1.302034  -0.727922   \n",
       "2 -0.678165  0.607294 -0.525668     ...        1.266874  0.063812  -0.678165   \n",
       "3 -1.035015  0.060759  1.485875     ...        1.266874  0.063812  -0.678165   \n",
       "4 -0.425962 -1.103060  1.378002     ...        0.703411  0.909991  -0.425962   \n",
       "\n",
       "   ymod3img  ymod4real  ymod4img  Unnamed: 21  Unnamed: 22  Unnamed: 23  \\\n",
       "0 -0.006522   0.372573 -1.747136          NaN          NaN          NaN   \n",
       "1 -0.006522   0.372573 -1.747136          NaN          NaN          NaN   \n",
       "2  0.607294  -1.035015  0.060759          NaN          NaN          NaN   \n",
       "3  0.607294  -1.035015  0.060759          NaN          NaN          NaN   \n",
       "4 -1.103060   3.286375  0.475154          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 24  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "important_features=['h1real' ,'h1imag' ,'h2real' ,'h2imag' ,'y1real' ,'y1imag' ,'y2real' ,'y2imag' ,'n1real' ,'n1imag' ,'n2real' ,'n2imag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SplitTrainAndTest(numberOfTraining, df):\n",
    "    msk1=np.random.rand(numberOfTraining)<1\n",
    "    msk2=np.random.rand(len(df)-numberOfTraining)<0\n",
    "    msk=np.concatenate((msk1,msk2), axis=0)\n",
    "    train=df[msk]\n",
    "    test=df[~msk]\n",
    "    return train, test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainData, testData=SplitTrainAndTest(800, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=trainData[important_features].values\n",
    "y_train=trainData['target'].values\n",
    "X_test=testData[important_features].values\n",
    "y_test=testData['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=X_train\n",
    "y=y_train\n",
    "num_examples = len(X) # training set size\n",
    "nn_input_dim = 12 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "# Gradient descent parameters (I picked these by hand)\n",
    "epsilon = 0.01 # learning rate for gradient descent\n",
    "reg_lambda = 0.01 # regularization strength\n",
    "# Helper function to evaluate the total loss on the dataset\n",
    "def calculate_loss(model):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation to calculate our predictions\n",
    "    z1 = X.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    # Calculating the loss\n",
    "    corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "    data_loss = np.sum(corect_logprobs)\n",
    "    # Add regulatization term to loss (optional)\n",
    "    data_loss += reg_lambda/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n",
    "    return 1./num_examples * data_loss\n",
    "# Helper function to predict an output (0 or 1)\n",
    "def predict(model, x):\n",
    "    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n",
    "    # Forward propagation\n",
    "    z1 = x.dot(W1) + b1\n",
    "    a1 = np.tanh(z1)\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    exp_scores = np.exp(z2)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)\n",
    "# This function learns parameters for the neural network and returns the model.\n",
    "# - nn_hdim: Number of nodes in the hidden layer\n",
    "# - num_passes: Number of passes through the training data for gradient descent\n",
    "# - print_loss: If True, print the loss every 1000 iterations\n",
    "def build_model(nn_hdim, num_passes=200, print_loss=False):   \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    \n",
    "    # Gradient descent. For each batch...\n",
    "    for i in xrange(0, num_passes):\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        \n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 1000 == 0:\n",
    "          print \"Loss after iteration %i: %f\" %(i, calculate_loss(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 3.059621\n",
      "Loss after iteration 200: 0.237820\n",
      "Loss after iteration 300: 0.040986\n",
      "Loss after iteration 400: 0.039630\n",
      "Loss after iteration 500: 0.038714\n",
      "Loss after iteration 600: 0.037938\n",
      "Loss after iteration 700: 0.037230\n",
      "Loss after iteration 800: 0.036564\n",
      "Loss after iteration 900: 0.035927\n",
      "Loss after iteration 1000: 0.035313\n",
      "Loss after iteration 1100: 0.034717\n",
      "Loss after iteration 1200: 0.034138\n",
      "Loss after iteration 1300: 0.033574\n",
      "Loss after iteration 1400: 0.033024\n",
      "Loss after iteration 1500: 0.032488\n",
      "Loss after iteration 1600: 0.031963\n",
      "Loss after iteration 1700: 0.031451\n",
      "Loss after iteration 1800: 0.030951\n",
      "Loss after iteration 1900: 0.030462\n",
      "Loss after iteration 2000: 0.029984\n",
      "Loss after iteration 2100: 0.029517\n",
      "Loss after iteration 2200: 0.029060\n",
      "Loss after iteration 2300: 0.028614\n",
      "Loss after iteration 2400: 0.028178\n",
      "Loss after iteration 2500: 0.027752\n",
      "Loss after iteration 2600: 0.027335\n",
      "Loss after iteration 2700: 0.026926\n",
      "Loss after iteration 2800: 0.026526\n",
      "Loss after iteration 2900: 0.026135\n",
      "Loss after iteration 3000: 0.025751\n",
      "Loss after iteration 3100: 0.025375\n",
      "Loss after iteration 3200: 0.025006\n",
      "Loss after iteration 3300: 0.024645\n",
      "Loss after iteration 3400: 0.024291\n",
      "Loss after iteration 3500: 0.023943\n",
      "Loss after iteration 3600: 0.023602\n",
      "Loss after iteration 3700: 0.023268\n",
      "Loss after iteration 3800: 0.022940\n",
      "Loss after iteration 3900: 0.022618\n",
      "Loss after iteration 4000: 0.022302\n",
      "Loss after iteration 4100: 0.021992\n",
      "Loss after iteration 4200: 0.021687\n",
      "Loss after iteration 4300: 0.021388\n",
      "Loss after iteration 4400: 0.021095\n",
      "Loss after iteration 4500: 0.020808\n",
      "Loss after iteration 4600: 0.020526\n",
      "Loss after iteration 4700: 0.020251\n",
      "Loss after iteration 4800: 0.019982\n",
      "Loss after iteration 4900: 0.019719\n",
      "Loss after iteration 5000: 0.019462\n",
      "Loss after iteration 5100: 0.019211\n",
      "Loss after iteration 5200: 0.018965\n",
      "Loss after iteration 5300: 0.018725\n",
      "Loss after iteration 5400: 0.018490\n",
      "Loss after iteration 5500: 0.018260\n",
      "Loss after iteration 5600: 0.018035\n",
      "Loss after iteration 5700: 0.017815\n",
      "Loss after iteration 5800: 0.017600\n",
      "Loss after iteration 5900: 0.017390\n",
      "Loss after iteration 6000: 0.017184\n",
      "Loss after iteration 6100: 0.016982\n",
      "Loss after iteration 6200: 0.016785\n",
      "Loss after iteration 6300: 0.016593\n",
      "Loss after iteration 6400: 0.016404\n",
      "Loss after iteration 6500: 0.016220\n",
      "Loss after iteration 6600: 0.016041\n",
      "Loss after iteration 6700: 0.015865\n",
      "Loss after iteration 6800: 0.015694\n",
      "Loss after iteration 6900: 0.015527\n",
      "Loss after iteration 7000: 0.015363\n",
      "Loss after iteration 7100: 0.015203\n",
      "Loss after iteration 7200: 0.015047\n",
      "Loss after iteration 7300: 0.014894\n",
      "Loss after iteration 7400: 0.014745\n",
      "Loss after iteration 7500: 0.014599\n",
      "Loss after iteration 7600: 0.014456\n",
      "Loss after iteration 7700: 0.014316\n",
      "Loss after iteration 7800: 0.014180\n",
      "Loss after iteration 7900: 0.014046\n",
      "Loss after iteration 8000: 0.013914\n",
      "Loss after iteration 8100: 0.013786\n",
      "Loss after iteration 8200: 0.013660\n",
      "Loss after iteration 8300: 0.013536\n",
      "Loss after iteration 8400: 0.013415\n",
      "Loss after iteration 8500: 0.013296\n",
      "Loss after iteration 8600: 0.013179\n",
      "Loss after iteration 8700: 0.013064\n",
      "Loss after iteration 8800: 0.012951\n",
      "Loss after iteration 8900: 0.012839\n",
      "Loss after iteration 9000: 0.012730\n",
      "Loss after iteration 9100: 0.012622\n",
      "Loss after iteration 9200: 0.012516\n",
      "Loss after iteration 9300: 0.012411\n",
      "Loss after iteration 9400: 0.012309\n",
      "Loss after iteration 9500: 0.012208\n",
      "Loss after iteration 9600: 0.012109\n",
      "Loss after iteration 9700: 0.012012\n",
      "Loss after iteration 9800: 0.011918\n",
      "Loss after iteration 9900: 0.011825\n",
      "Loss after iteration 10000: 0.011734\n",
      "Loss after iteration 10100: 0.011645\n",
      "Loss after iteration 10200: 0.011557\n",
      "Loss after iteration 10300: 0.011472\n",
      "Loss after iteration 10400: 0.011388\n",
      "Loss after iteration 10500: 0.011306\n",
      "Loss after iteration 10600: 0.011226\n",
      "Loss after iteration 10700: 0.011148\n",
      "Loss after iteration 10800: 0.011071\n",
      "Loss after iteration 10900: 0.010996\n",
      "Loss after iteration 11000: 0.010923\n",
      "Loss after iteration 11100: 0.010852\n",
      "Loss after iteration 11200: 0.010782\n",
      "Loss after iteration 11300: 0.010714\n",
      "Loss after iteration 11400: 0.010648\n",
      "Loss after iteration 11500: 0.010583\n",
      "Loss after iteration 11600: 0.010520\n",
      "Loss after iteration 11700: 0.010458\n",
      "Loss after iteration 11800: 0.010398\n",
      "Loss after iteration 11900: 0.010339\n",
      "Loss after iteration 12000: 0.010281\n",
      "Loss after iteration 12100: 0.010224\n",
      "Loss after iteration 12200: 0.010168\n",
      "Loss after iteration 12300: 0.010114\n",
      "Loss after iteration 12400: 0.010060\n",
      "Loss after iteration 12500: 0.010007\n",
      "--------Model 1 Trained Successfully !----------\n",
      "--------Accuracy For Model 1----------\n",
      "112\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model1=build_model_with_stopping_criteria(60,desired_loss=0.01, print_loss=True)\n",
    "print \"--------Model 1 Trained Successfully !----------\"\n",
    "prediction1=predict(model1, X_test)\n",
    "print \"--------Accuracy For Model 1----------\"\n",
    "get_accuracy(prediction1, y_test)\n",
    "print \"----------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, target):\n",
    "    num_correct=(prediction==target).sum()\n",
    "    print num_correct\n",
    "    accuracy=1.0*num_correct/len(target)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction1=predict(model1,X_test)\n",
    "prediction2=predict(model2,X_test)\n",
    "prediction3=predict(model3,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5472636815920398"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(prediction1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6169154228855721"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(prediction2, y_test)\n",
    "get_accuracy(prediction3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model_with_stopping_criteria(nn_hdim, desired_loss=0.01, print_loss=False):   \n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(nn_input_dim, nn_hdim) / np.sqrt(nn_input_dim)\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    W2 = np.random.randn(nn_hdim, nn_output_dim) / np.sqrt(nn_hdim)\n",
    "    b2 = np.zeros((1, nn_output_dim))\n",
    "    model_loss=1\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "    i=0\n",
    "    # Gradient descent. For each batch...\n",
    "    while model_loss>desired_loss:\n",
    "        i+=1\n",
    "\n",
    "        # Forward propagation\n",
    "        z1 = X.dot(W1) + b1\n",
    "        a1 = np.tanh(z1)\n",
    "        z2 = a1.dot(W2) + b2\n",
    "        exp_scores = np.exp(z2)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        # Backpropagation\n",
    "        delta3 = probs\n",
    "        delta3[range(num_examples), y] -= 1\n",
    "        dW2 = (a1.T).dot(delta3)\n",
    "        db2 = np.sum(delta3, axis=0, keepdims=True)\n",
    "        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n",
    "        dW1 = np.dot(X.T, delta2)\n",
    "        db1 = np.sum(delta2, axis=0)\n",
    "\n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        dW2 += reg_lambda * W2\n",
    "        dW1 += reg_lambda * W1\n",
    "\n",
    "        # Gradient descent parameter update\n",
    "        W1 += -epsilon * dW1\n",
    "        b1 += -epsilon * db1\n",
    "        W2 += -epsilon * dW2\n",
    "        b2 += -epsilon * db2\n",
    "        \n",
    "        # Assign new parameters to the model\n",
    "        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n",
    "        model_loss=calculate_loss(model)\n",
    "        # Optionally print the loss.\n",
    "        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
    "        if print_loss and i % 100 == 0:\n",
    "          print \"Loss after iteration %i: %f\" %(i, calculate_loss(model))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 4.631361\n",
      "Loss after iteration 200: 0.054485\n",
      "Loss after iteration 300: 0.048081\n",
      "Loss after iteration 400: 0.046809\n",
      "Loss after iteration 500: 0.045811\n",
      "Loss after iteration 600: 0.044907\n",
      "Loss after iteration 700: 0.044055\n",
      "Loss after iteration 800: 0.043238\n",
      "Loss after iteration 900: 0.042448\n",
      "Loss after iteration 1000: 0.041682\n",
      "Loss after iteration 1100: 0.040936\n",
      "Loss after iteration 1200: 0.040208\n",
      "Loss after iteration 1300: 0.039499\n",
      "Loss after iteration 1400: 0.038806\n",
      "Loss after iteration 1500: 0.038130\n",
      "Loss after iteration 1600: 0.037469\n",
      "Loss after iteration 1700: 0.036823\n",
      "Loss after iteration 1800: 0.036192\n",
      "Loss after iteration 1900: 0.035575\n",
      "Loss after iteration 2000: 0.034972\n",
      "Loss after iteration 2100: 0.034381\n",
      "Loss after iteration 2200: 0.033804\n",
      "Loss after iteration 2300: 0.033239\n",
      "Loss after iteration 2400: 0.032687\n",
      "Loss after iteration 2500: 0.032146\n",
      "Loss after iteration 2600: 0.031617\n",
      "Loss after iteration 2700: 0.031099\n",
      "Loss after iteration 2800: 0.030593\n",
      "Loss after iteration 2900: 0.030097\n",
      "Loss after iteration 3000: 0.029611\n",
      "Loss after iteration 3100: 0.029136\n",
      "Loss after iteration 3200: 0.028670\n",
      "Loss after iteration 3300: 0.028214\n",
      "Loss after iteration 3400: 0.027768\n",
      "Loss after iteration 3500: 0.027330\n",
      "Loss after iteration 3600: 0.026901\n",
      "Loss after iteration 3700: 0.026481\n",
      "Loss after iteration 3800: 0.026069\n",
      "Loss after iteration 3900: 0.025665\n",
      "Loss after iteration 4000: 0.025270\n",
      "Loss after iteration 4100: 0.024882\n",
      "Loss after iteration 4200: 0.024502\n",
      "Loss after iteration 4300: 0.024129\n",
      "Loss after iteration 4400: 0.023764\n",
      "Loss after iteration 4500: 0.023407\n",
      "Loss after iteration 4600: 0.023057\n",
      "Loss after iteration 4700: 0.022714\n",
      "Loss after iteration 4800: 0.022378\n",
      "Loss after iteration 4900: 0.022049\n",
      "Loss after iteration 5000: 0.021727\n",
      "Loss after iteration 5100: 0.021412\n",
      "Loss after iteration 5200: 0.021104\n",
      "Loss after iteration 5300: 0.020803\n",
      "Loss after iteration 5400: 0.020507\n",
      "Loss after iteration 5500: 0.020219\n",
      "Loss after iteration 5600: 0.019936\n",
      "Loss after iteration 5700: 0.019661\n",
      "Loss after iteration 5800: 0.019391\n",
      "Loss after iteration 5900: 0.019127\n",
      "Loss after iteration 6000: 0.018870\n",
      "Loss after iteration 6100: 0.018619\n",
      "Loss after iteration 6200: 0.018373\n",
      "Loss after iteration 6300: 0.018134\n",
      "Loss after iteration 6400: 0.017900\n",
      "Loss after iteration 6500: 0.017671\n",
      "Loss after iteration 6600: 0.017449\n",
      "Loss after iteration 6700: 0.017231\n",
      "Loss after iteration 6800: 0.017019\n",
      "Loss after iteration 6900: 0.016811\n",
      "Loss after iteration 7000: 0.016609\n",
      "Loss after iteration 7100: 0.016411\n",
      "Loss after iteration 7200: 0.016219\n",
      "Loss after iteration 7300: 0.016030\n",
      "Loss after iteration 7400: 0.015846\n",
      "Loss after iteration 7500: 0.015667\n",
      "Loss after iteration 7600: 0.015492\n",
      "Loss after iteration 7700: 0.015320\n",
      "Loss after iteration 7800: 0.015153\n",
      "Loss after iteration 7900: 0.014989\n",
      "Loss after iteration 8000: 0.014829\n",
      "Loss after iteration 8100: 0.014673\n",
      "Loss after iteration 8200: 0.014519\n",
      "Loss after iteration 8300: 0.014370\n",
      "Loss after iteration 8400: 0.014223\n",
      "Loss after iteration 8500: 0.014079\n",
      "Loss after iteration 8600: 0.013939\n",
      "Loss after iteration 8700: 0.013801\n",
      "Loss after iteration 8800: 0.013667\n",
      "Loss after iteration 8900: 0.013535\n",
      "Loss after iteration 9000: 0.013405\n",
      "Loss after iteration 9100: 0.013278\n",
      "Loss after iteration 9200: 0.013154\n",
      "Loss after iteration 9300: 0.013032\n",
      "Loss after iteration 9400: 0.012913\n",
      "Loss after iteration 9500: 0.012795\n",
      "Loss after iteration 9600: 0.012680\n",
      "Loss after iteration 9700: 0.012567\n",
      "Loss after iteration 9800: 0.012456\n",
      "Loss after iteration 9900: 0.012346\n",
      "Loss after iteration 10000: 0.012239\n",
      "Loss after iteration 10100: 0.012134\n",
      "Loss after iteration 10200: 0.012030\n",
      "Loss after iteration 10300: 0.011928\n",
      "Loss after iteration 10400: 0.011828\n",
      "Loss after iteration 10500: 0.011730\n",
      "Loss after iteration 10600: 0.011634\n",
      "Loss after iteration 10700: 0.011539\n",
      "Loss after iteration 10800: 0.011446\n",
      "Loss after iteration 10900: 0.011356\n",
      "Loss after iteration 11000: 0.011267\n",
      "Loss after iteration 11100: 0.011180\n",
      "Loss after iteration 11200: 0.011095\n",
      "Loss after iteration 11300: 0.011012\n",
      "Loss after iteration 11400: 0.010931\n",
      "Loss after iteration 11500: 0.010852\n",
      "Loss after iteration 11600: 0.010775\n",
      "Loss after iteration 11700: 0.010699\n",
      "Loss after iteration 11800: 0.010626\n",
      "Loss after iteration 11900: 0.010554\n",
      "Loss after iteration 12000: 0.010483\n",
      "Loss after iteration 12100: 0.010415\n",
      "Loss after iteration 12200: 0.010348\n",
      "Loss after iteration 12300: 0.010282\n",
      "Loss after iteration 12400: 0.010218\n",
      "Loss after iteration 12500: 0.010155\n",
      "Loss after iteration 12600: 0.010094\n",
      "Loss after iteration 12700: 0.010034\n"
     ]
    }
   ],
   "source": [
    "model31=build_model_with_stopping_criteria(60,desired_loss=0.01, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction31=predict(model31, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5422885572139303"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(prediction31, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 2.642174\n",
      "Loss after iteration 200: 0.054657\n",
      "Loss after iteration 300: 0.039668\n",
      "Loss after iteration 400: 0.037906\n",
      "Loss after iteration 500: 0.036923\n",
      "Loss after iteration 600: 0.036145\n",
      "Loss after iteration 700: 0.035455\n",
      "Loss after iteration 800: 0.034815\n",
      "Loss after iteration 900: 0.034209\n",
      "Loss after iteration 1000: 0.033629\n",
      "Loss after iteration 1100: 0.033070\n",
      "Loss after iteration 1200: 0.032528\n",
      "Loss after iteration 1300: 0.032001\n",
      "Loss after iteration 1400: 0.031488\n",
      "Loss after iteration 1500: 0.030988\n",
      "Loss after iteration 1600: 0.030500\n",
      "Loss after iteration 1700: 0.030022\n",
      "Loss after iteration 1800: 0.029556\n",
      "Loss after iteration 1900: 0.029100\n",
      "Loss after iteration 2000: 0.028654\n",
      "Loss after iteration 2100: 0.028218\n",
      "Loss after iteration 2200: 0.027792\n",
      "Loss after iteration 2300: 0.027375\n",
      "Loss after iteration 2400: 0.026967\n",
      "Loss after iteration 2500: 0.026568\n",
      "Loss after iteration 2600: 0.026177\n",
      "Loss after iteration 2700: 0.025795\n",
      "Loss after iteration 2800: 0.025421\n",
      "Loss after iteration 2900: 0.025055\n",
      "Loss after iteration 3000: 0.024697\n",
      "Loss after iteration 3100: 0.024346\n",
      "Loss after iteration 3200: 0.024002\n",
      "Loss after iteration 3300: 0.023666\n",
      "Loss after iteration 3400: 0.023336\n",
      "Loss after iteration 3500: 0.023012\n",
      "Loss after iteration 3600: 0.022695\n",
      "Loss after iteration 3700: 0.022384\n",
      "Loss after iteration 3800: 0.022079\n",
      "Loss after iteration 3900: 0.021780\n",
      "Loss after iteration 4000: 0.021485\n",
      "Loss after iteration 4100: 0.021197\n",
      "Loss after iteration 4200: 0.020913\n",
      "Loss after iteration 4300: 0.020634\n",
      "Loss after iteration 4400: 0.020360\n",
      "Loss after iteration 4500: 0.020092\n",
      "Loss after iteration 4600: 0.019828\n",
      "Loss after iteration 4700: 0.019569\n",
      "Loss after iteration 4800: 0.019316\n",
      "Loss after iteration 4900: 0.019068\n",
      "Loss after iteration 5000: 0.018826\n",
      "Loss after iteration 5100: 0.018588\n",
      "Loss after iteration 5200: 0.018357\n",
      "Loss after iteration 5300: 0.018130\n",
      "Loss after iteration 5400: 0.017908\n",
      "Loss after iteration 5500: 0.017692\n",
      "Loss after iteration 5600: 0.017481\n",
      "Loss after iteration 5700: 0.017275\n",
      "Loss after iteration 5800: 0.017074\n",
      "Loss after iteration 5900: 0.016878\n",
      "Loss after iteration 6000: 0.016687\n",
      "Loss after iteration 6100: 0.016501\n",
      "Loss after iteration 6200: 0.016319\n",
      "Loss after iteration 6300: 0.016142\n",
      "Loss after iteration 6400: 0.015969\n",
      "Loss after iteration 6500: 0.015800\n",
      "Loss after iteration 6600: 0.015635\n",
      "Loss after iteration 6700: 0.015475\n",
      "Loss after iteration 6800: 0.015318\n",
      "Loss after iteration 6900: 0.015164\n",
      "Loss after iteration 7000: 0.015014\n",
      "Loss after iteration 7100: 0.014867\n",
      "Loss after iteration 7200: 0.014724\n",
      "Loss after iteration 7300: 0.014584\n",
      "Loss after iteration 7400: 0.014447\n",
      "Loss after iteration 7500: 0.014312\n",
      "Loss after iteration 7600: 0.014181\n",
      "Loss after iteration 7700: 0.014053\n",
      "Loss after iteration 7800: 0.013927\n",
      "Loss after iteration 7900: 0.013804\n",
      "Loss after iteration 8000: 0.013684\n",
      "Loss after iteration 8100: 0.013566\n",
      "Loss after iteration 8200: 0.013450\n",
      "Loss after iteration 8300: 0.013337\n",
      "Loss after iteration 8400: 0.013225\n",
      "Loss after iteration 8500: 0.013115\n",
      "Loss after iteration 8600: 0.013007\n",
      "Loss after iteration 8700: 0.012901\n",
      "Loss after iteration 8800: 0.012796\n",
      "Loss after iteration 8900: 0.012692\n",
      "Loss after iteration 9000: 0.012590\n",
      "Loss after iteration 9100: 0.012490\n",
      "Loss after iteration 9200: 0.012391\n",
      "Loss after iteration 9300: 0.012293\n",
      "Loss after iteration 9400: 0.012198\n",
      "Loss after iteration 9500: 0.012103\n",
      "Loss after iteration 9600: 0.012011\n",
      "Loss after iteration 9700: 0.011920\n",
      "Loss after iteration 9800: 0.011831\n",
      "Loss after iteration 9900: 0.011744\n",
      "Loss after iteration 10000: 0.011659\n",
      "Loss after iteration 10100: 0.011575\n",
      "Loss after iteration 10200: 0.011494\n",
      "Loss after iteration 10300: 0.011414\n",
      "Loss after iteration 10400: 0.011336\n",
      "Loss after iteration 10500: 0.011260\n",
      "Loss after iteration 10600: 0.011186\n",
      "Loss after iteration 10700: 0.011113\n",
      "Loss after iteration 10800: 0.011043\n",
      "Loss after iteration 10900: 0.010974\n",
      "Loss after iteration 11000: 0.010907\n",
      "Loss after iteration 11100: 0.010842\n",
      "Loss after iteration 11200: 0.010778\n",
      "Loss after iteration 11300: 0.010716\n",
      "Loss after iteration 11400: 0.010655\n",
      "Loss after iteration 11500: 0.010595\n",
      "Loss after iteration 11600: 0.010537\n",
      "Loss after iteration 11700: 0.010480\n",
      "Loss after iteration 11800: 0.010425\n",
      "Loss after iteration 11900: 0.010370\n",
      "Loss after iteration 12000: 0.010317\n",
      "Loss after iteration 12100: 0.010264\n",
      "Loss after iteration 12200: 0.010213\n",
      "Loss after iteration 12300: 0.010162\n",
      "Loss after iteration 12400: 0.010112\n",
      "Loss after iteration 12500: 0.010063\n",
      "Loss after iteration 12600: 0.010015\n",
      "112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model32=build_model_with_stopping_criteria(60,desired_loss=0.01, print_loss=True)\n",
    "prediction32=predict(model32, X_test)\n",
    "get_accuracy(prediction32, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction32=predict(model32, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6218905472636815"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(prediction32, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 100: 3.274432\n",
      "Loss after iteration 200: 0.840677\n",
      "Loss after iteration 300: 0.037666\n",
      "Loss after iteration 400: 0.035806\n",
      "Loss after iteration 500: 0.034765\n",
      "Loss after iteration 600: 0.033974\n",
      "Loss after iteration 700: 0.033295\n",
      "Loss after iteration 800: 0.032680\n",
      "Loss after iteration 900: 0.032106\n",
      "Loss after iteration 1000: 0.031561\n",
      "Loss after iteration 1100: 0.031039\n",
      "Loss after iteration 1200: 0.030535\n",
      "Loss after iteration 1300: 0.030047\n",
      "Loss after iteration 1400: 0.029572\n",
      "Loss after iteration 1500: 0.029108\n",
      "Loss after iteration 1600: 0.028656\n",
      "Loss after iteration 1700: 0.028215\n",
      "Loss after iteration 1800: 0.027783\n",
      "Loss after iteration 1900: 0.027360\n",
      "Loss after iteration 2000: 0.026947\n",
      "Loss after iteration 2100: 0.026542\n",
      "Loss after iteration 2200: 0.026147\n",
      "Loss after iteration 2300: 0.025759\n",
      "Loss after iteration 2400: 0.025380\n",
      "Loss after iteration 2500: 0.025009\n",
      "Loss after iteration 2600: 0.024645\n",
      "Loss after iteration 2700: 0.024289\n",
      "Loss after iteration 2800: 0.023940\n",
      "Loss after iteration 2900: 0.023598\n",
      "Loss after iteration 3000: 0.023262\n",
      "Loss after iteration 3100: 0.022933\n",
      "Loss after iteration 3200: 0.022611\n",
      "Loss after iteration 3300: 0.022294\n",
      "Loss after iteration 3400: 0.021984\n",
      "Loss after iteration 3500: 0.021680\n",
      "Loss after iteration 3600: 0.021382\n",
      "Loss after iteration 3700: 0.021090\n",
      "Loss after iteration 3800: 0.020804\n",
      "Loss after iteration 3900: 0.020524\n",
      "Loss after iteration 4000: 0.020251\n",
      "Loss after iteration 4100: 0.019983\n",
      "Loss after iteration 4200: 0.019722\n",
      "Loss after iteration 4300: 0.019466\n",
      "Loss after iteration 4400: 0.019216\n",
      "Loss after iteration 4500: 0.018972\n",
      "Loss after iteration 4600: 0.018733\n",
      "Loss after iteration 4700: 0.018499\n",
      "Loss after iteration 4800: 0.018271\n",
      "Loss after iteration 4900: 0.018048\n",
      "Loss after iteration 5000: 0.017830\n",
      "Loss after iteration 5100: 0.017617\n",
      "Loss after iteration 5200: 0.017408\n",
      "Loss after iteration 5300: 0.017205\n",
      "Loss after iteration 5400: 0.017005\n",
      "Loss after iteration 5500: 0.016811\n",
      "Loss after iteration 5600: 0.016620\n",
      "Loss after iteration 5700: 0.016434\n",
      "Loss after iteration 5800: 0.016251\n",
      "Loss after iteration 5900: 0.016073\n",
      "Loss after iteration 6000: 0.015899\n",
      "Loss after iteration 6100: 0.015729\n",
      "Loss after iteration 6200: 0.015562\n",
      "Loss after iteration 6300: 0.015399\n",
      "Loss after iteration 6400: 0.015240\n",
      "Loss after iteration 6500: 0.015084\n",
      "Loss after iteration 6600: 0.014932\n",
      "Loss after iteration 6700: 0.014783\n",
      "Loss after iteration 6800: 0.014638\n",
      "Loss after iteration 6900: 0.014495\n",
      "Loss after iteration 7000: 0.014356\n",
      "Loss after iteration 7100: 0.014219\n",
      "Loss after iteration 7200: 0.014085\n",
      "Loss after iteration 7300: 0.013953\n",
      "Loss after iteration 7400: 0.013823\n",
      "Loss after iteration 7500: 0.013696\n",
      "Loss after iteration 7600: 0.013572\n",
      "Loss after iteration 7700: 0.013449\n",
      "Loss after iteration 7800: 0.013328\n",
      "Loss after iteration 7900: 0.013210\n",
      "Loss after iteration 8000: 0.013094\n",
      "Loss after iteration 8100: 0.012980\n",
      "Loss after iteration 8200: 0.012868\n",
      "Loss after iteration 8300: 0.012758\n",
      "Loss after iteration 8400: 0.012651\n",
      "Loss after iteration 8500: 0.012545\n",
      "Loss after iteration 8600: 0.012442\n",
      "Loss after iteration 8700: 0.012341\n",
      "Loss after iteration 8800: 0.012242\n",
      "Loss after iteration 8900: 0.012145\n",
      "Loss after iteration 9000: 0.012050\n",
      "Loss after iteration 9100: 0.011957\n",
      "Loss after iteration 9200: 0.011866\n",
      "Loss after iteration 9300: 0.011777\n",
      "Loss after iteration 9400: 0.011689\n",
      "Loss after iteration 9500: 0.011602\n",
      "Loss after iteration 9600: 0.011518\n",
      "Loss after iteration 9700: 0.011434\n",
      "Loss after iteration 9800: 0.011352\n",
      "Loss after iteration 9900: 0.011272\n",
      "Loss after iteration 10000: 0.011193\n",
      "Loss after iteration 10100: 0.011115\n",
      "Loss after iteration 10200: 0.011039\n",
      "Loss after iteration 10300: 0.010965\n",
      "Loss after iteration 10400: 0.010893\n",
      "Loss after iteration 10500: 0.010823\n",
      "Loss after iteration 10600: 0.010754\n",
      "Loss after iteration 10700: 0.010687\n",
      "Loss after iteration 10800: 0.010622\n",
      "Loss after iteration 10900: 0.010559\n",
      "Loss after iteration 11000: 0.010498\n",
      "Loss after iteration 11100: 0.010438\n",
      "Loss after iteration 11200: 0.010380\n",
      "Loss after iteration 11300: 0.010323\n",
      "Loss after iteration 11400: 0.010269\n",
      "Loss after iteration 11500: 0.010216\n",
      "Loss after iteration 11600: 0.010164\n",
      "Loss after iteration 11700: 0.010114\n",
      "Loss after iteration 11800: 0.010066\n",
      "Loss after iteration 11900: 0.010019\n",
      "99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4925373134328358"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model33=build_model_with_stopping_criteria(55,desired_loss=0.01, print_loss=True)\n",
    "prediction33=predict(model33, X_test)\n",
    "get_accuracy(prediction33, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
